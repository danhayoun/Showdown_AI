{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*On va regarder comment améliorer la fonction de reward*\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répertoire actuel : /Users/dan2/Desktop/Télécom-master-spé/Projets_perso/Deep/Showdown_AI/my_showdown_ai_git\n"
     ]
    }
   ],
   "source": [
    "#1 on se place \n",
    "import os\n",
    "import ipynbname\n",
    "\n",
    "chemin_notebook = ipynbname.path()\n",
    "dossier_notebook = os.path.dirname(chemin_notebook)\n",
    "os.chdir(dossier_notebook)\n",
    "os.chdir('../..')\n",
    "print(\"Répertoire actuel :\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poke_env.player.random_player import RandomPlayer\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import json\n",
    "from poke_env.environment.pokemon import Pokemon\n",
    "import poke_env\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from poke_env.data import GenData\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "from poke_env.player.random_player import RandomPlayer\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from poke_env.player import Gen4EnvSinglePlayer\n",
    "from poke_env.data import GenData\n",
    "import torch\n",
    "from gymnasium.spaces import Space, Box, Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Players.Player5 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_WEATHER = 2\n",
    "LEN_POKE = 16 \n",
    "LEN_ACTIVE_POKE = 23 \n",
    "MAX_LEN_TEAM_SWITCHES = 80 #\n",
    "IDX_ACTION_SPACE_SIZE = 232\n",
    "class adapted_obs :\n",
    "    def __init__(self,battle) :\n",
    "        self.dict_observation = battle.current_observation.__dict__ \n",
    "        #et stocker l'état précédent ? Pour l'instant on passe\n",
    "        self.side_conditions = np.array(get_side_conditions(self.dict_observation,1),dtype=np.float32) # padding et normalisation déjà réalisés dedans\n",
    "\n",
    "        self.opponent_side_conditions = np.array(get_side_conditions(self.dict_observation,0),dtype=np.float32) # padding et normalisation déjà réalisés dedans \n",
    "        \n",
    "        self.weather = get_weather(self.dict_observation)\n",
    "        self.weather[0] = weather_to_id[self.weather[0]]/4\n",
    "        self.weather = pad_list(self.weather,max_len =MAX_LEN_WEATHER) # padding \n",
    "        self.weather = np.array(self.weather,dtype=np.float32)\n",
    "\n",
    "        self.active_pokemon =np.array( ObservedPokemon_to_list(self.dict_observation[\"active_pokemon\"],is_active_pokemon=1,is_opponent_team=0),dtype=np.float32)\n",
    "        self.my_team_switches = np.array(get_my_switches_with_all_infos(battle.available_switches,self.dict_observation),dtype=np.float32)\n",
    "        #Calcul du action space avant le padding\n",
    "        lock = is_my_pokemon_locked(self.dict_observation)\n",
    "        self.action_space_size = len(self.my_team_switches)//16 + lock*1 + (1-lock)*4 #SI lock alors 1 move, sinon 4\n",
    "        self.action_space_size = self.action_space_size / 9\n",
    "        self.action_space_size = np.array([self.action_space_size], dtype=np.float32)\n",
    "        self.my_team_switches = pad_vector(self.my_team_switches,max_len= MAX_LEN_TEAM_SWITCHES)\n",
    "        self.opponent_active_pokemon = np.array(ObservedPokemon_to_list(self.dict_observation[\"opponent_active_pokemon\"],is_active_pokemon=1,is_opponent_team=1),dtype=np.float32) #à 1 car on veut savoir si il est boosté\n",
    "        \n",
    "        #Equipe adverse sans le pokemon actif \n",
    "        self.opponent_team = np.array(get_opponent_team_with_all_infos(self.dict_observation,battle),dtype=np.float32)\n",
    "        self.opponent_team = pad_vector(self.opponent_team ,max_len= MAX_LEN_TEAM_SWITCHES)\n",
    "        #Fin équipe adverse \n",
    "        self.output = np.concatenate([\n",
    "            self.side_conditions, #Fonctionnel\n",
    "            self.opponent_side_conditions, #Fonctionnel\n",
    "            self.weather, #Fonctionnel\n",
    "            self.active_pokemon, #Fonctionnel\n",
    "            self.my_team_switches, #Fonctionnel\n",
    "            self.opponent_active_pokemon, #Fonctionnel\n",
    "            self.opponent_team, #Fonctionnel\n",
    "            self.action_space_size #Fonctionnel\n",
    "        ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## DEFINIR L'EMBEDDING ##################\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "# Initialiser GenData pour la génération souhaitée (par exemple, génération 4)\n",
    "gen_data = GenData.from_gen(4)\n",
    "\n",
    "# Accéder au tableau des types\n",
    "type_chart = gen_data.type_chart\n",
    "\n",
    "\n",
    "from poke_env.player.env_player import Gen4EnvSinglePlayer\n",
    "import numpy as np\n",
    "import time, random\n",
    "\n",
    "class Player5Training(Gen4EnvSinglePlayer):\n",
    "    def __init__(self, model, battle_format=\"gen4randombattle\",max_concurrent_battles=1, **kwargs):\n",
    "        self._max_concurrent_battles = max_concurrent_battles\n",
    "        super().__init__(battle_format=battle_format,**kwargs)\n",
    "        self._init_environment()\n",
    "        self.model = model\n",
    "        self.action_space = Discrete(9)\n",
    "        self.my_last_moves_ids = deque(maxlen=3)\n",
    "        self.opp_last_moves_ids = deque(maxlen=3)\n",
    "        self.moves_to_id = moves_to_id  # important si moves_to_id est global\n",
    "\n",
    "    def _init_environment(self):\n",
    "            # Hack pour forcer poke-env à prendre en compte le bon nombre de battles\n",
    "            self._n_battles = self._max_concurrent_battles\n",
    "\n",
    "    def embed_battle(self, battle):\n",
    "        # Remplace ça par ta propre obs\n",
    "        obs = adapted_obs(battle).output\n",
    "        return obs\n",
    "\n",
    "    def describe_embedding(self):\n",
    "        return Box(low=0, high=256, shape=(233,), dtype=np.float32)\n",
    "    \n",
    "    def update_last_moves(self, battle):\n",
    "        if battle.move_turn:\n",
    "            move = battle.move_turn[0].move\n",
    "            if move:\n",
    "                move_id = self.moves_to_id.get(move.id, None)\n",
    "                if move_id is not None:\n",
    "                    self.my_last_moves.append(move_id)\n",
    "\n",
    "        if battle.opponent_active_pokemon and battle.opponent_active_pokemon.moves:\n",
    "            opp_move = list(battle.opponent_active_pokemon.moves.values())[0]\n",
    "            if opp_move:\n",
    "                opp_move_id = self.moves_to_id.get(opp_move.id, None)\n",
    "                if opp_move_id is not None:\n",
    "                    self.opp_last_moves.append(opp_move_id)\n",
    "\n",
    "    \n",
    "    def _get_boosting_moves_for_stat(self, stat):\n",
    "        boosting_moves = {\n",
    "            \"atk\": [\"swordsdance\", \"bulkup\"],\n",
    "            \"spa\": [\"nastyplot\", \"calmmind\"],\n",
    "            \"def\": [\"irondefense\"],\n",
    "            \"spd\": [\"amnesia\"],\n",
    "            \"spe\": [\"agility\", \"rockpolish\"],\n",
    "        }\n",
    "        return boosting_moves.get(stat, [])\n",
    "    \n",
    "    def count_recent_boosts(self, n_turns=3):\n",
    "        count = 0\n",
    "        recent_moves = list(self.my_last_moves_ids)[-n_turns:]\n",
    "        boost_moves = [\n",
    "            move\n",
    "            for stat in [\"atk\", \"spa\", \"def\", \"spd\", \"spe\"]\n",
    "            for move in self._get_boosting_moves_for_stat(stat)\n",
    "        ]\n",
    "\n",
    "        for move_id in recent_moves:\n",
    "            move_name = next((k for k, v in self.moves_to_id.items() if v == move_id), None)\n",
    "            if move_name in boost_moves:\n",
    "                count += 1\n",
    "\n",
    "        return count\n",
    "\n",
    "    def calc_reward(self, last_battle, current_battle):\n",
    "        reward = self.reward_computing_helper(\n",
    "            current_battle, fainted_value=2.0, hp_value=1.0, victory_value=30.0\n",
    "        )\n",
    "\n",
    "        my_poke = current_battle.active_pokemon\n",
    "        opp_poke = current_battle.opponent_active_pokemon\n",
    "\n",
    "        # Derniers coups (ids entiers)\n",
    "        last_move_id = self.my_last_moves_ids[-1] if self.my_last_moves_ids else None\n",
    "        prev_move_id = self.my_last_moves_ids[-2] if len(self.my_last_moves_ids) >= 2 else None\n",
    "        last_opp_move_id = self.opp_last_moves_ids[-1] if self.opp_last_moves_ids else None\n",
    "\n",
    "        # ========== Double Protect pénalisé ==========\n",
    "        protect_id = self.moves_to_id.get(\"protect\", 182 + 1)\n",
    "        if last_move_id == protect_id and prev_move_id == protect_id:\n",
    "            if opp_poke.status in [\"psn\", \"tox\", \"brn\"]:\n",
    "                if opp_poke.current_hp < opp_poke.max_hp:\n",
    "                    reward += 0.5\n",
    "                else:\n",
    "                    reward -= 2.0\n",
    "            else:\n",
    "                reward -= 3.0\n",
    "\n",
    "        # ========== Bonus si Abri contre un switch adverse ==========\n",
    "        if last_move_id == protect_id and current_battle.opponent_switch:\n",
    "            reward += 2.0\n",
    "\n",
    "        # ========== Pénalité si attaque sur immunité ==========\n",
    "        if last_move_id is not None:\n",
    "            move_name = [k for k, v in self.moves_to_id.items() if v == last_move_id]\n",
    "            if move_name:\n",
    "                move_obj = self._get_move_by_id(move_name[0], current_battle)\n",
    "                if move_obj and move_obj.base_power > 0:\n",
    "                    effectiveness = move_obj.type.damage_multiplier(opp_poke.type_1, opp_poke.type_2)\n",
    "                    if effectiveness == 0:\n",
    "                        if move_obj.base_power >= 60:  # équiv. à last_move_damage >= 0.3\n",
    "                            reward += 5.0  # anticipation (ex: double switch)\n",
    "                        else:\n",
    "                            reward -= 4.0\n",
    "\n",
    "        # ========== Pénalité si boost sur stat déjà à +6 ==========\n",
    "        for stat, boost in my_poke.boosts.items():\n",
    "            if boost >= 6:\n",
    "                for move_name, move_id in self.moves_to_id.items():\n",
    "                    if move_id == last_move_id and move_name in self._get_boosting_moves_for_stat(stat):\n",
    "                        reward -= 2.0\n",
    "                        break\n",
    "\n",
    "        # ========== Surboost suicidaire : faint juste après 2 boosts ==========\n",
    "        if my_poke.fainted:\n",
    "            nb_boosts = self.count_recent_boosts(n_turns=3)\n",
    "            if nb_boosts >= 2:\n",
    "                factor = 3 if nb_boosts >= 3 else 2\n",
    "                reward -= factor * 2.5\n",
    "\n",
    "        # ========== Pose de pièges ==========\n",
    "        turn = current_battle.turn\n",
    "        stealthrock_id = self.moves_to_id.get(\"stealthrock\", 446 + 1)\n",
    "        spikes_id = self.moves_to_id.get(\"spikes\", 191 + 1)\n",
    "\n",
    "        if last_move_id == stealthrock_id:\n",
    "            if current_battle.fields[\"stealthrock\"]:\n",
    "                reward -= 2.0\n",
    "            else:\n",
    "                reward += 4.0 if turn <= 15 else 2.0\n",
    "\n",
    "        if last_move_id == spikes_id:\n",
    "            n_spikes = current_battle.fields.get(\"spikes\", 0)\n",
    "            if n_spikes >= 3:\n",
    "                reward -= 2.0\n",
    "            else:\n",
    "                reward += 4.0 if turn <= 15 else 2.0\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def action_to_move(self, action, battle):\n",
    "        return super().action_to_move(action, battle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.dqn.policies import DQNPolicy\n",
    "from stable_baselines3.common.torch_layers import create_mlp\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "class CustomDQNPolicySGD(DQNPolicy):\n",
    "    def __init__(self, observation_space, action_space, lr_schedule, **kwargs):\n",
    "        kwargs[\"net_arch\"] = [512,256,128,64,64,32]\n",
    "        super().__init__(observation_space, action_space, lr_schedule, **kwargs)\n",
    "    def make_optimizers(self) -> None:\n",
    "        # Redéfinir l'optimiseur ici\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.lr_schedule(1),  # learning_rate peut être une fonction\n",
    "            momentum=0.9,            # optionnel\n",
    "            weight_decay=0.0         # optionnel\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Réinstancier l'env\n",
    "opponent = RandomPlayer(battle_format=\"gen4randombattle\")\n",
    "\n",
    "train_env_raw = Player5Training(\n",
    "    model = None,\n",
    "    battle_format=\"gen4randombattle\",\n",
    "    opponent=opponent,\n",
    "    max_concurrent_battles=10\n",
    ")\n",
    "\n",
    "train_env = DummyVecEnv([lambda: train_env_raw])  # Redéfini bien après reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "#env = train_env\n",
    "\n",
    "model = DQN(\n",
    "    policy=CustomDQNPolicySGD, #A la base : CustomDQNPolicy\n",
    "    env=train_env,\n",
    "    learning_rate=2.5e-4,\n",
    "    buffer_size=10000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=32,\n",
    "    gamma=0.5,\n",
    "    train_freq=8,\n",
    "    target_update_interval=1,\n",
    "    exploration_fraction=1.0,\n",
    "    exploration_final_eps=0.05,\n",
    "    verbose=1,\n",
    "    device=\"mps\",\n",
    "    _init_setup_model=True #intialement à False quand on prenait des envs custom\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x164410e50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.reset()\n",
    "model.learn(total_timesteps=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "showdown_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33e7e3701f48ebda16bcd6f46fe6335779fe7b4b5fe34aacb01951541666ec3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
