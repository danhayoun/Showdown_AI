{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Player4 avec embedding des types avanc√© (graphes) (m√™me si prend pas en compte le c√¥t√© offensif), stats, nb pok√©mons vivants/morts***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©pertoire actuel : /Users/dan2/Desktop/TeÃÅleÃÅcom-master-speÃÅ/Projets_perso/Deep/Showdown_AI/my_showdown_ai_git\n"
     ]
    }
   ],
   "source": [
    "#1 on se place \n",
    "import os\n",
    "import ipynbname\n",
    "\n",
    "chemin_notebook = ipynbname.path()\n",
    "dossier_notebook = os.path.dirname(chemin_notebook)\n",
    "os.chdir(dossier_notebook)\n",
    "os.chdir('../..')\n",
    "print(\"R√©pertoire actuel :\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.embedding.type.type_embedding import *\n",
    "from Players.Player4 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## DEFINIR L'EMBEDDING ##################\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Space, Box\n",
    "from poke_env.player import Gen4EnvSinglePlayer\n",
    "from poke_env.data import GenData\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "import torch\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "\n",
    "# Initialiser GenData pour la g√©n√©ration souhait√©e (par exemple, g√©n√©ration 4)\n",
    "gen_data = GenData.from_gen(4)\n",
    "\n",
    "# Acc√©der au tableau des types\n",
    "type_chart = gen_data.type_chart\n",
    "\n",
    "\n",
    "class Player4Training(Gen4EnvSinglePlayer): #Classe pour entrainer le mod√®le 4. \n",
    "    def __init__(self,model_path=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if model_path is not None :\n",
    "            #self.model = DQN.load(model_path, device=\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "            print(f\"üì• Mod√®le charg√© depuis {model_path}\")\n",
    "\n",
    "        self.action_space = Discrete(9)  # ‚úÖ attribut classique\n",
    "        self.my_team = None\n",
    "        self.my_team_index = {}\n",
    "        self.opponent_team = None\n",
    "        self.opponent_team_index = {}\n",
    "    \n",
    "    #Toujours m√™mes valeurs de reward\n",
    "    def calc_reward(self, last_battle, current_battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            current_battle, fainted_value=2.0, hp_value=1.0, victory_value=30.0\n",
    "        )\n",
    "\n",
    "    def embed_battle(self, battle)  :\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        moves_real_power = -np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "                moves_real_power[i] = moves_dmg_multiplier[i]*moves_base_power[i]\n",
    "\n",
    "        if battle.turn == 1:\n",
    "            self.my_team, self.my_team_index = init_my_team(battle)\n",
    "            self.opponent_team, self.opponent_team_index = init_opponent_team(battle)\n",
    "        else : \n",
    "            self.my_team = update_my_team(battle, self.my_team, self.my_team_index)\n",
    "            self.opponent_team, self.opponent_team_index = update_opponent_team(battle, self.opponent_team, self.opponent_team_index)\n",
    "\n",
    "        \n",
    "\n",
    "        # Final vector with 10 components\n",
    "        final_vector = np.concatenate(\n",
    "            [\n",
    "                moves_real_power,\n",
    "                self.my_team,\n",
    "                self.opponent_team\n",
    "            ]\n",
    "        )\n",
    "        final_tensor = torch.tensor(final_vector,dtype=torch.float32)\n",
    "        #print(\"taille tensor state : \",final_tensor.shape)\n",
    "        #print(\"index mon √©quipe : \",self.my_team_index)\n",
    "        #print(\"index equipe adverse : \",self.opponent_team_index)\n",
    "        return final_tensor\n",
    "\n",
    "    def describe_embedding(self) -> Space:\n",
    "        low = (\n",
    "            [-1] * 4 +          # real power\n",
    "            [0] * 72 +         # my team types\n",
    "            [0] * 72           # opponent team types\n",
    "        )\n",
    "        high = (\n",
    "            [3] * 4 +           # real power\n",
    "            [1] * 72 +         # my team types\n",
    "            [1] * 72           # opponent team types\n",
    "        )\n",
    "\n",
    "        return Box(\n",
    "            np.array(low, dtype=np.float32),\n",
    "            np.array(high, dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def action_to_move(self, action: int, battle: AbstractBattle):\n",
    "        order = super().action_to_move(action, battle)\n",
    "        return order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DEFINIR MODELE ##################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#R√©seau perceptron √† deux couche cach√©e, sortie lin√©aire, f activation = Relu,\n",
    "class DQNModel(nn.Module):\n",
    "    def __init__(self, input_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.out = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNModelAdversaire(nn.Module):\n",
    "    def __init__(self, input_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2= nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.out = nn.Linear(32, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        real_power = x[:4]\n",
    "        types = x[4:] \n",
    "        real_power_padded = F.pad(real_power, (0, 14))  # [17]\n",
    "        real_power_padded = real_power_padded.unsqueeze(0)  # [1, 17] \n",
    "        sequence = torch.cat([real_power_padded, types], dim=0)\n",
    "        sequence = sequence.T.unsqueeze(0)     \n",
    "        types = types.view(12, 17) \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#opponent = AdversarialTrainPlayer(battle_format=\"gen4randombattle\")\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# R√©instancier l'env\n",
    "train_env_raw = Player4Training(\n",
    "    battle_format=\"gen4randombattle\",\n",
    "    opponent=RandomPlayer(battle_format=\"gen4randombattle\"),\n",
    "    start_challenging=True\n",
    ")\n",
    "\n",
    "train_env = DummyVecEnv([lambda: train_env_raw])  # Red√©fini bien apr√®s reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 18:12:44,324 - RandomPlayer 1 - CRITICAL - Error message received: |nametaken|RandomPlayer 1|Someone is already using the name \"RandomPlayer 1\".\n",
      "2025-04-14 18:12:44,324 - RandomPlayer 1 - ERROR - Unhandled exception raised while handling message:\n",
      "|nametaken|RandomPlayer 1|Someone is already using the name \"RandomPlayer 1\".\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dan2/Desktop/T√©l√©com-master-sp√©/Projets_perso/Deep/Showdown_AI/my_showdown_ai_git/showdown_4/lib/python3.11/site-packages/poke_env/ps_client/ps_client.py\", line 164, in _handle_message\n",
      "    raise ShowdownException(\"Error message received: %s\", message)\n",
      "poke_env.exceptions.ShowdownException: ('Error message received: %s', '|nametaken|RandomPlayer 1|Someone is already using the name \"RandomPlayer 1\".')\n"
     ]
    }
   ],
   "source": [
    "env = train_env\n",
    "\n",
    "model = DQN(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=2.5e-4,\n",
    "    buffer_size=10000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=32,\n",
    "    gamma=0.5,\n",
    "    train_freq=1,\n",
    "    target_update_interval=1,\n",
    "    exploration_fraction=1.0,\n",
    "    exploration_final_eps=0.05,\n",
    "    policy_kwargs=dict(activation_fn=nn.ReLU, net_arch=[128, 64])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#steps = 1000000\n",
    "#callback = CustomTQDMCallback(total_timesteps = steps, check_freq=1000, verbose=0)\n",
    "#model.learn(\n",
    "#    total_timesteps=steps,\n",
    "#    callback=callback\n",
    "#)\n",
    "#model.save(\"Players/player_4_dynamic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player4EvalvsBot(Gen4EnvSinglePlayer):\n",
    "\n",
    "    def __init__(self, model_path=\"Players/player_4\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = DQN.load(model_path, device=\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        print(f\"üì• Mod√®le charg√© depuis {model_path}\")\n",
    "        print(\"R√©pertoire actuel :\", os.getcwd())\n",
    "        self.my_team = None\n",
    "        self.my_team_index = {}\n",
    "        self.opponent_team = None\n",
    "        self.opponent_team_index = {}\n",
    "        \n",
    "        #Toujours m√™mes valeurs de reward\n",
    "    def calc_reward(self, last_battle, current_battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            current_battle, fainted_value=2.0, hp_value=1.0, victory_value=30.0\n",
    "        )\n",
    "\n",
    "    def embed_battle(self, battle)  :\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        moves_real_power = -np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "                moves_real_power[i] = moves_dmg_multiplier[i]*moves_base_power[i]\n",
    "\n",
    "        if battle.turn == 1:\n",
    "            self.my_team, self.my_team_index = init_my_team(battle)\n",
    "            self.opponent_team, self.opponent_team_index = init_opponent_team(battle)\n",
    "        else : \n",
    "            self.my_team = update_my_team(battle, self.my_team, self.my_team_index)\n",
    "            self.opponent_team, self.opponent_team_index = update_opponent_team(battle, self.opponent_team, self.opponent_team_index)\n",
    "\n",
    "        \n",
    "\n",
    "        # Final vector with 10 components\n",
    "        final_vector = np.concatenate(\n",
    "            [\n",
    "                moves_real_power,\n",
    "                self.my_team,\n",
    "                self.opponent_team\n",
    "            ]\n",
    "        )\n",
    "        final_tensor = torch.tensor(final_vector,dtype=torch.float32)\n",
    "        return final_tensor\n",
    "\n",
    "    def describe_embedding(self) -> Space:\n",
    "        low = (\n",
    "            [-1] * 4 +          # real power\n",
    "            [0] * 72 +         # my team types\n",
    "            [0] * 72           # opponent team types\n",
    "        )\n",
    "        high = (\n",
    "            [3] * 4 +           # real power\n",
    "            [1] * 72 +         # my team types\n",
    "            [1] * 72           # opponent team types\n",
    "        )\n",
    "\n",
    "        return Box(\n",
    "            np.array(low, dtype=np.float32),\n",
    "            np.array(high, dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def action_to_move(self, action, battle):\n",
    "        print('my_team_index : ',self.my_team_index)\n",
    "        moves = battle.available_moves\n",
    "        switches = battle.available_switches\n",
    "        total_actions = len(moves) + len(switches)\n",
    "\n",
    "        #print(f\"üî¢ DQN ‚Üí action={action} | #moves={len(moves)} | #switches={len(switches)} | total={total_actions}\")\n",
    "\n",
    "        if 0 <= action < len(moves):\n",
    "            return self.create_order(moves[action])\n",
    "        elif len(moves) <= action < total_actions:\n",
    "            return self.create_order(switches[action - len(moves)])\n",
    "        else:\n",
    "            #print(\"‚ùå Action hors bornes, choix d'un move al√©atoire\")\n",
    "            return self.choose_random_move(battle)\n",
    "        \n",
    "    def predict(self, obs):\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        q_values = self.model.q_net(obs_tensor)\n",
    "        print(f\"üìä Q-values : {q_values.detach().numpy().flatten()}\")\n",
    "        action = int(torch.argmax(q_values).item())\n",
    "        return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***EVAL***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TypeAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=17, out_features=12, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=12, out_features=4, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=12, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=12, out_features=17, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############DEFINIR L ADVERSAIRE#####################\n",
    "from Utils.embedding.old.old_embedding import *\n",
    "#Instancier l'encodeur du type \n",
    "from Utils.embedding.old.type_autoencodeur import TypeAutoencoder\n",
    "\n",
    "encodeur_type = TypeAutoencoder(encoded_size=4)\n",
    "\n",
    "# Charger les poids sauvegard√©s\n",
    "encodeur_type.load_state_dict(torch.load(\"Utils/embedding/old/type_autoencoder.pth\", map_location='mps'))\n",
    "encodeur_type.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialTrainPlayer(RandomPlayer): # Adversaire de Player4 lors de l'entrainement\n",
    "\n",
    "    def __init__(self, model_path=\"Players/embedding_2\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = DQN.load(model_path, device=\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        print(f\"üì• Mod√®le charg√© depuis {model_path}\")\n",
    "\n",
    "    def embed_battle(self, battle )  :\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        moves_real_power = -np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "                moves_real_power[i] = moves_dmg_multiplier[i]*moves_base_power[i]\n",
    "\n",
    "        pokemon_types_compressed = []\n",
    "        #Pokemon types  \n",
    "        pokemon_types = obtain_pokemon_types(battle)\n",
    "        pokemon_types = torch.tensor(obtain_pokemon_types(battle)).view(12, 17)  # 6 pok√©s par team = 12 vecteurs\n",
    "        with torch.no_grad():\n",
    "            for i in range(12) :\n",
    "                vec = pokemon_types[i].unsqueeze(0)\n",
    "                encoded = encodeur_type.encoder(vec.float()) \n",
    "                pokemon_types_compressed.append(encoded.squeeze(0))\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        compressed_flat = torch.cat(pokemon_types_compressed).numpy()\n",
    "        final_vector = np.concatenate(\n",
    "            [\n",
    "                moves_real_power,\n",
    "                compressed_flat,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return np.float32(final_vector)\n",
    "    \n",
    "    #action_to_move suppr\n",
    "        \n",
    "    def predict(self, obs):\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        q_values = self.model.q_net(obs_tensor)\n",
    "        print(f\"üìä Q-values : {q_values.detach().numpy().flatten()}\")\n",
    "        action = int(torch.argmax(q_values).item())\n",
    "        return action\n",
    "    \n",
    "    def choose_move(self, battle):\n",
    "        obs = self.embed_battle(battle)\n",
    "        device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model.q_net(obs_tensor)\n",
    "            action = int(torch.argmax(q_values).item())\n",
    "\n",
    "        moves = battle.available_moves\n",
    "        switches = battle.available_switches\n",
    "        total_actions = len(moves) + len(switches)\n",
    "\n",
    "        if 0 <= action < len(moves):\n",
    "            return self.create_order(moves[action])\n",
    "        elif len(moves) <= action < total_actions:\n",
    "            return self.create_order(switches[action - len(moves)])\n",
    "        else:\n",
    "            return self.choose_random_move(battle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Mod√®le charg√© depuis Players/embedding_2\n",
      "üì• Mod√®le charg√© depuis Players/player_4\n",
      "R√©pertoire actuel : /Users/dan2/Desktop/TeÃÅleÃÅcom-master-speÃÅ/Projets_perso/Deep/Showdown_AI/my_showdown_ai_git\n"
     ]
    }
   ],
   "source": [
    "opponent = AdversarialTrainPlayer(battle_format=\"gen4randombattle\")\n",
    "eval_env = Player4EvalvsBot(\n",
    "    battle_format=\"gen4randombattle\", opponent=opponent, start_challenging=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277c5c47659848a6ab9d977402e7cbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n",
      "my_team_index :  {'metagross': 0, 'primeape': 1, 'snorlax': 2, 'vileplume': 3, 'seaking': 4, 'mothim': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 0 victoires sur 1 matchs\n",
      "üéØ Reward moyen : -35.56\n"
     ]
    }
   ],
   "source": [
    "n_eval_episodes = 1\n",
    "rewards = []\n",
    "wins = 0\n",
    "\n",
    "obs, _ = eval_env.reset()\n",
    "for _ in tqdm(range(n_eval_episodes)):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        total_reward += reward\n",
    "        if done and reward > 0:\n",
    "            wins += 1\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "    obs, _ = eval_env.reset()\n",
    "\n",
    "print(f\"‚úÖ {wins} victoires sur {n_eval_episodes} matchs\")\n",
    "print(f\"üéØ Reward moyen : {sum(rewards) / len(rewards):.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour que je puisse l'affronter : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.DQ_simple import DQ_simple \n",
    "class Player4vsHuman(DQ_simple) :\n",
    "    def __init__(self, model_path = \"Players/player_4\", battle_format=\"gen4randombattle\"):\n",
    "        super().__init__(battle_format=battle_format)\n",
    "        \n",
    "        self.model = DQN.load(model_path, device=\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        print(\"üì• Mod√®le DQN charg√© :\", self.model)\n",
    "        self.my_team = None\n",
    "        self.my_team_index = {}\n",
    "        self.opponent_team = None\n",
    "        self.opponent_team_index = {}\n",
    "    \n",
    "    def embed_battle(self, battle)  :\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        moves_real_power = -np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "                moves_real_power[i] = moves_dmg_multiplier[i]*moves_base_power[i]\n",
    "\n",
    "        if battle.turn == 1:\n",
    "            self.my_team, self.my_team_index = init_my_team(battle)\n",
    "            self.opponent_team, self.opponent_team_index = init_opponent_team(battle)\n",
    "        else : \n",
    "            self.my_team = update_my_team(battle, self.my_team, self.my_team_index)\n",
    "            self.opponent_team, self.opponent_team_index = update_opponent_team(battle, self.opponent_team, self.opponent_team_index)\n",
    "\n",
    "        \n",
    "\n",
    "        # Final vector with 10 components\n",
    "        final_vector = np.concatenate(\n",
    "            [\n",
    "                moves_real_power,\n",
    "                self.my_team,\n",
    "                self.opponent_team\n",
    "            ]\n",
    "        )\n",
    "        final_tensor = torch.tensor(final_vector,dtype=torch.float32)\n",
    "        return final_tensor\n",
    "\n",
    "    def choose_move(self, battle):\n",
    "        print(\"üëâ nouveau tour !\")\n",
    "        print(\"√©tat de ma team : \",self.my_team)\n",
    "        print(\"√©tat de la team adverse : \",self.opponent_team)\n",
    "\n",
    "        # Obtenir l'observation de l'√©tat du combat\n",
    "        obs = self.embed_battle(battle)\n",
    "        #print(\"üìä Observation de l'√©tat :\", obs)\n",
    "\n",
    "        # Pr√©dire l'action avec le mod√®le DQN\n",
    "        action = int(self.model.predict(obs, deterministic=True)[0])\n",
    "        \n",
    "        #print(\"üéØ Action choisie par DQN :\", action)\n",
    "        moves = battle.available_moves\n",
    "        switches = battle.available_switches\n",
    "        total_actions = len(moves) + len(switches)\n",
    "        self.create_order.dynamax = False\n",
    "\n",
    "        if 0 <= action < len(moves):\n",
    "            print(self.create_order(moves[action]))\n",
    "            return self.create_order(moves[action])\n",
    "        elif len(moves) <= action < total_actions:\n",
    "            print(self.create_order(switches[action - len(moves)]))\n",
    "            return self.create_order(switches[action - len(moves)])\n",
    "        else:\n",
    "            print(\"XXXX   : Random Moove \")\n",
    "            a = self.choose_random_move(battle)\n",
    "            print(a)\n",
    "            return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Mod√®le DQN charg√© : <stable_baselines3.dqn.dqn.DQN object at 0x175c18b50>\n",
      "üì• Mod√®le DQN charg√© : <stable_baselines3.dqn.dqn.DQN object at 0x3157059d0>\n"
     ]
    }
   ],
   "source": [
    "bot_player = Player4vsHuman()\n",
    "\n",
    "#await bot_player.send_challenges(\"chewba\", n_challenges=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "showdown_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33e7e3701f48ebda16bcd6f46fe6335779fe7b4b5fe34aacb01951541666ec3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
