{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import poke_env\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ne sais pas vraiment sur quels types d'encodages partir donc on va en essayer plusieurs.\n",
    "Ce qui est s√ªr c'est que je veux prendre en compte les pok√©mons en questions jou√©s, leurs mooves, leur speed, leurs stats, plus tard leurs objets.\n",
    "\n",
    "La question c'est comment on repr√©sente toutes ces valeurs qui ne sont pas continues ? (id pok√©mon/type, moovepool)\n",
    "\n",
    "On va essayer diff√©rentes m√©thodes : \n",
    "\n",
    "1) on ne prend pas en compte les ids des pok√©s, juste leurs types. Pour les mooves, on prend en compte le type et la puissance. Pour repr√©senter le type : one-hot encoding. donc on aura 1 one-hot encoding pour le type du pok√©, puis 1 pour chacun de ses mooves. Comment relier √ßa √† la table des types ? On va la recr√©er sous forme de matrice. Emplacement de chaque type (normal = ligne/colonne 1, fire = ligne/colonne 2 etc ..) =\n",
    "\n",
    "{normal, fire, water, electric, grass, ice, fighting, poison, ground, flying, psychic, bug, rock, ghost, dragon, dark, steel, fairy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.5 0.  1.  1.  0.5 1. ]\n",
      " [1.  0.5 0.5 1.  2.  2.  1.  1.  1.  1.  1.  2.  0.5 1.  0.5 1.  2.  1. ]\n",
      " [1.  2.  0.5 1.  0.5 1.  1.  1.  2.  1.  1.  1.  2.  1.  0.5 1.  1.  1. ]\n",
      " [1.  1.  2.  0.5 0.5 1.  1.  1.  0.  2.  1.  1.  1.  1.  0.5 1.  1.  1. ]\n",
      " [1.  0.5 2.  1.  0.5 1.  1.  0.5 2.  0.5 1.  0.5 2.  1.  0.5 1.  0.5 1. ]\n",
      " [1.  0.5 0.5 1.  2.  0.5 1.  1.  2.  2.  1.  1.  1.  1.  2.  1.  0.5 1. ]\n",
      " [2.  1.  1.  1.  1.  2.  1.  0.5 1.  0.5 0.5 0.5 2.  0.  1.  2.  2.  0.5]\n",
      " [1.  1.  1.  1.  2.  1.  1.  0.5 0.5 1.  1.  1.  0.5 0.5 1.  1.  0.  2. ]\n",
      " [1.  2.  1.  2.  0.5 1.  1.  2.  1.  0.  1.  0.5 2.  1.  1.  1.  2.  1. ]\n",
      " [1.  1.  1.  0.5 2.  1.  2.  1.  1.  1.  1.  2.  0.5 1.  1.  1.  0.5 1. ]\n",
      " [1.  1.  1.  1.  1.  1.  2.  2.  1.  1.  0.5 1.  1.  1.  1.  0.  0.5 1. ]\n",
      " [1.  0.5 1.  1.  2.  1.  0.5 0.5 1.  0.5 2.  1.  1.  0.5 1.  2.  0.5 0.5]\n",
      " [1.  2.  1.  1.  1.  2.  0.5 1.  0.5 2.  1.  2.  1.  1.  1.  1.  0.5 1. ]\n",
      " [0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  1.  1.  2.  1.  0.5 1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  1.  0.5 0. ]\n",
      " [1.  1.  1.  1.  1.  1.  0.5 1.  1.  1.  2.  1.  1.  2.  1.  0.5 1.  0.5]\n",
      " [1.  0.5 0.5 0.5 1.  2.  1.  1.  1.  1.  1.  1.  2.  1.  1.  1.  0.5 2. ]\n",
      " [1.  0.5 1.  1.  1.  1.  2.  0.5 1.  1.  1.  1.  1.  1.  2.  2.  0.5 1. ]]\n"
     ]
    }
   ],
   "source": [
    "#Table des types, forme matricielle\n",
    "type_list = [\n",
    "    \"normal\", \"fire\", \"water\", \"electric\", \"grass\", \"ice\", \"fighting\",\n",
    "    \"poison\", \"ground\", \"flying\", \"psychic\", \"bug\", \"rock\", \"ghost\",\n",
    "    \"dragon\", \"dark\", \"steel\", \"fairy\"\n",
    "]\n",
    "\n",
    "# Matrice des multiplicateurs d'apr√®s le tableau officiel des types Pok√©mon\n",
    "type_chart = {\n",
    "    \"normal\":   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.5, 0, 1, 1, 0.5, 1],\n",
    "    \"fire\":     [1, 0.5, 0.5, 1, 2, 2, 1, 1, 1, 1, 1, 2, 0.5, 1, 0.5, 1, 2, 1],\n",
    "    \"water\":    [1, 2, 0.5, 1, 0.5, 1, 1, 1, 2, 1, 1, 1, 2, 1, 0.5, 1, 1, 1],\n",
    "    \"electric\": [1, 1, 2, 0.5, 0.5, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0.5, 1, 1, 1],\n",
    "    \"grass\":    [1, 0.5, 2, 1, 0.5, 1, 1, 0.5, 2, 0.5, 1, 0.5, 2, 1, 0.5, 1, 0.5, 1],\n",
    "    \"ice\":      [1, 0.5, 0.5, 1, 2, 0.5, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 0.5, 1],\n",
    "    \"fighting\": [2, 1, 1, 1, 1, 2, 1, 0.5, 1, 0.5, 0.5, 0.5, 2, 0, 1, 2, 2, 0.5],\n",
    "    \"poison\":   [1, 1, 1, 1, 2, 1, 1, 0.5, 0.5, 1, 1, 1, 0.5, 0.5, 1, 1, 0, 2],\n",
    "    \"ground\":   [1, 2, 1, 2, 0.5, 1, 1, 2, 1, 0, 1, 0.5, 2, 1, 1, 1, 2, 1],\n",
    "    \"flying\":   [1, 1, 1, 0.5, 2, 1, 2, 1, 1, 1, 1, 2, 0.5, 1, 1, 1, 0.5, 1],\n",
    "    \"psychic\":  [1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 0.5, 1, 1, 1, 1, 0, 0.5, 1],\n",
    "    \"bug\":      [1, 0.5, 1, 1, 2, 1, 0.5, 0.5, 1, 0.5, 2, 1, 1, 0.5, 1, 2, 0.5, 0.5],\n",
    "    \"rock\":     [1, 2, 1, 1, 1, 2, 0.5, 1, 0.5, 2, 1, 2, 1, 1, 1, 1, 0.5, 1],\n",
    "    \"ghost\":    [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 0.5, 1, 1],\n",
    "    \"dragon\":   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0.5, 0],\n",
    "    \"dark\":     [1, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 2, 1, 1, 2, 1, 0.5, 1, 0.5],\n",
    "    \"steel\":    [1, 0.5, 0.5, 0.5, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0.5, 2],\n",
    "    \"fairy\":    [1, 0.5, 1, 1, 1, 1, 2, 0.5, 1, 1, 1, 1, 1, 1, 2, 2, 0.5, 1],\n",
    "}\n",
    "\n",
    "df_type_chart = pd.DataFrame(type_chart, index=type_list).T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "type_list = [\n",
    "    \"normal\", \"fire\", \"water\", \"electric\", \"grass\", \"ice\", \"fighting\",\n",
    "    \"poison\", \"ground\", \"flying\", \"psychic\", \"bug\", \"rock\", \"ghost\",\n",
    "    \"dragon\", \"dark\", \"steel\", \"fairy\"\n",
    "]\n",
    "\n",
    "type_to_idx = {t: i for i, t in enumerate(type_list)}\n",
    "\n",
    "# Convertir le DataFrame pr√©c√©dent en matrice NumPy\n",
    "type_matrix = df_type_chart.to_numpy()\n",
    "\n",
    "print(type_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi le vecteur 'type normal' est : [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "type qui attaque = sur la ligne, type qui re√ßoit = sur la colonne"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On prend en compte le type de chaque pok√©mon, pour que des corr√©lations puissent se faire sur les switchs. Donc dans mon embedd j'aurais les types des 5 autres pok√©s."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on va tester un premier embedding ! \n",
    "On est oblig√©s d'avoir un vecteur 1D donc il va falloir se dire o√π est-ce qu'on met quoi :\n",
    "\n",
    "Embedding = ['moves_base_power*moves_dmg_multiplier','pokemon_types']\n",
    "\n",
    "Rq : ne prendra pas en compte les pokemons sans types (l'attaque feu qui fait perdre le type l√†, bref...). peut-√™tre cr√©er une 19√®me colonne ? colonne No_type genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonctions n√©cessaires\n",
    "def obtain_one_hot_vector_type(p) :\n",
    "    \"\"\"renvoie le type du pokemon au format one-hot encoding\n",
    "    entr√©e : poke_env.environment.pokemon.Pokemon\n",
    "    sortie : np.array, de longueur fix√©e 18\"\"\"\n",
    "    vec = np.zeros(18)\n",
    "\n",
    "    if p.type_1 : \n",
    "        vec[type_to_idx[p.type_1.name.lower()]] = 1.0\n",
    "    if p.type_2 :\n",
    "        vec[type_to_idx[p.type_2.name.lower()]] = 1.0\n",
    "    return vec\n",
    "\n",
    "def obtain_pokemon_types(battle):\n",
    "\n",
    "    # Encode ta team\n",
    "    vectors_my_team = [\n",
    "        obtain_one_hot_vector_type(p) if not p.fainted else np.zeros(len(type_to_idx), dtype=np.float32)\n",
    "        for _, p in battle.team.items()\n",
    "    ]\n",
    "    while len(vectors_my_team) < 6:\n",
    "        vectors_my_team.append(np.zeros(len(type_to_idx), dtype=np.float32))\n",
    "    my_team_type = np.concatenate(vectors_my_team)\n",
    "\n",
    "    # Encode la team adverse\n",
    "    vectors_opponent_team = [\n",
    "        obtain_one_hot_vector_type(p) if not p.fainted else np.zeros(len(type_to_idx), dtype=np.float32)\n",
    "        for _, p in battle.opponent_team.items()\n",
    "    ]\n",
    "    while len(vectors_opponent_team) < 6:\n",
    "        vectors_opponent_team.append(np.zeros(len(type_to_idx), dtype=np.float32))\n",
    "    opponent_team_type = np.concatenate(vectors_opponent_team)\n",
    "\n",
    "    # Concat√®ne les deux parties\n",
    "    pokemon_types = np.concatenate([my_team_type, opponent_team_type])\n",
    "    \n",
    "    return pokemon_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium.spaces import Space, Box\n",
    "from poke_env.player import Gen8EnvSinglePlayer\n",
    "from poke_env.data import GenData\n",
    "import numpy as np\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "import torch\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "\n",
    "# Initialiser GenData pour la g√©n√©ration souhait√©e (par exemple, g√©n√©ration 8)\n",
    "gen_data = GenData.from_gen(8)\n",
    "\n",
    "# Acc√©der au tableau des types\n",
    "type_chart = gen_data.type_chart\n",
    "\n",
    "\n",
    "class embedding_Player(Gen8EnvSinglePlayer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.action_space = Discrete(9)  # ‚úÖ attribut classique\n",
    "    \n",
    "    #Toujours m√™mes valeurs de reward\n",
    "    def calc_reward(self, last_battle, current_battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            current_battle, fainted_value=2.0, hp_value=1.0, victory_value=30.0\n",
    "        )\n",
    "\n",
    "    def embed_battle(self, battle )  :\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        moves_real_power = -np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "                moves_real_power[i] = moves_dmg_multiplier[i]*moves_base_power[i]\n",
    "\n",
    "\n",
    "        #Pokemon types  \n",
    "        pokemon_types = obtain_pokemon_types(battle)\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        final_vector = np.concatenate(\n",
    "            [\n",
    "                moves_real_power,\n",
    "                pokemon_types,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return np.float32(final_vector)\n",
    "\n",
    "    def describe_embedding(self) -> Space:\n",
    "        low = (\n",
    "            [-1] * 4 +          # real power\n",
    "            [0] * 108 +         # my team types\n",
    "            [0] * 108           # opponent team types\n",
    "        )\n",
    "        high = (\n",
    "            [3] * 4 +           # real power\n",
    "            [1] * 108 +         # my team types\n",
    "            [1] * 108           # opponent team types\n",
    "        )\n",
    "\n",
    "        return Box(\n",
    "            np.array(low, dtype=np.float32),\n",
    "            np.array(high, dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def action_to_move(self, action: int, battle: AbstractBattle):\n",
    "        order = super().action_to_move(action, battle)\n",
    "        order.dynamax = False  # üî• d√©sactive Dynamax pour toutes les actions\n",
    "        return order\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DEBUG\n",
    "from poke_env.environment.pokemon import Pokemon\n",
    "\n",
    "# TEST obtain_one_hot_vector_type(p) ###\n",
    "p = Pokemon(gen=8, species=\"abomasnow\")\n",
    "#print(type(p))\n",
    "#print(p.species)    # 'abomasnow'\n",
    "#print((p.type_1.name.lower()))   # 'grass'\n",
    "#print(p.type_2)\n",
    "\n",
    "vec = np.zeros(18)\n",
    "\n",
    "if p.type_1 : \n",
    "    vec[type_to_idx[p.type_1.name.lower()]] = 1.0\n",
    "if p.type_2 :\n",
    "    vec[type_to_idx[p.type_2.name.lower()]] = 1.0\n",
    "\n",
    "#print(vec)\n",
    "\n",
    "\n",
    "# TEST obtain_pokemon_types(battle) ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poke_env.player.random_player import RandomPlayer\n",
    "\n",
    "class NoDynamaxRandomPlayer(RandomPlayer):\n",
    "    def choose_move(self, battle):\n",
    "        choice = super().choose_move(battle)\n",
    "        if choice.dynamax:\n",
    "            choice.dynamax = False  # d√©sactive l'option\n",
    "        return choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TRAINING***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opponent = NoDynamaxRandomPlayer(battle_format=\"gen8randombattle\")\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# instanciation du player\n",
    "train_env_raw = embedding_Player(\n",
    "    battle_format=\"gen8randombattle\",\n",
    "    opponent=RandomPlayer(battle_format=\"gen8randombattle\"),\n",
    "    start_challenging=True\n",
    ")\n",
    "\n",
    "# wrap dans DummyVecEnv (SB3 attend un vecteur d‚Äôenvs, m√™me pour un seul)\n",
    "train_env = DummyVecEnv([lambda: train_env_raw])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#R√©seau perceptron √† une couche cach√©e, sortie lin√©aire, f activation = Relu,\n",
    "class DQNModel(nn.Module):\n",
    "    def __init__(self, input_dim, n_actions):\n",
    "        super(DQNModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.out = nn.Linear(64, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Si ton env est d√©j√† Gym-compatible (comme avec poke-env Gym wrapper)\n",
    "env = train_env\n",
    "\n",
    "model = DQN(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=2.5e-4,\n",
    "    buffer_size=10000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=32,\n",
    "    gamma=0.5,\n",
    "    train_freq=1,\n",
    "    target_update_interval=1,\n",
    "    exploration_fraction=1.0,\n",
    "    exploration_final_eps=0.05,\n",
    "    policy_kwargs=dict(activation_fn=nn.ReLU, net_arch=[128, 64])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "class CustomTQDMCallback(BaseCallback):\n",
    "    def __init__(self, total_timesteps, check_freq=500, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.progress_bar = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.progress_bar = tqdm(total=self.total_timesteps, desc=\"üìä Training progress\")\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        # Avance la barre\n",
    "        if self.progress_bar:\n",
    "            self.progress_bar.update(1)\n",
    "\n",
    "        # Affichage toutes les X √©tapes\n",
    "        if self.n_calls % self.check_freq == 0 and self.verbose:\n",
    "            self.progress_bar.set_postfix_str(f\"Timesteps: {self.num_timesteps}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        if self.progress_bar:\n",
    "            self.progress_bar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e3332fd96d4e6694f0ffa7df4726ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üìä Training progress:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = 50000\n",
    "callback = CustomTQDMCallback(total_timesteps = steps, check_freq=1000, verbose=1)\n",
    "model.learn(\n",
    "    total_timesteps=steps,\n",
    "    callback=callback\n",
    ")\n",
    "model.save(\"embedding_1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TEST***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "class EmbeddingTestPlayer(Gen8EnvSinglePlayer):\n",
    "\n",
    "    def __init__(self, model_path=\"embedding_1\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = DQN.load(model_path, device=\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        print(f\"üì• Mod√®le charg√© depuis {model_path}\")\n",
    "\n",
    "        #Toujours m√™mes valeurs de reward\n",
    "    def calc_reward(self, last_battle, current_battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            current_battle, fainted_value=2.0, hp_value=1.0, victory_value=30.0\n",
    "        )\n",
    "\n",
    "    def embed_battle(self, battle )  :\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        moves_real_power = -np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "                moves_real_power[i] = moves_dmg_multiplier[i]*moves_base_power[i]\n",
    "\n",
    "\n",
    "        #Pokemon types  \n",
    "        pokemon_types = obtain_pokemon_types(battle)\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        final_vector = np.concatenate(\n",
    "            [\n",
    "                moves_real_power,\n",
    "                pokemon_types,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return np.float32(final_vector)\n",
    "\n",
    "    def describe_embedding(self) -> Space:\n",
    "        low = (\n",
    "            [-1] * 4 +          # real power\n",
    "            [0] * 108 +         # my team types\n",
    "            [0] * 108           # opponent team types\n",
    "        )\n",
    "        high = (\n",
    "            [3] * 4 +           # real power\n",
    "            [1] * 108 +         # my team types\n",
    "            [1] * 108           # opponent team types\n",
    "        )\n",
    "\n",
    "        return Box(\n",
    "            np.array(low, dtype=np.float32),\n",
    "            np.array(high, dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def action_to_move(self, action, battle):\n",
    "        moves = battle.available_moves\n",
    "        switches = battle.available_switches\n",
    "        total_actions = len(moves) + len(switches)\n",
    "\n",
    "        #print(f\"üî¢ DQN ‚Üí action={action} | #moves={len(moves)} | #switches={len(switches)} | total={total_actions}\")\n",
    "\n",
    "        if 0 <= action < len(moves):\n",
    "            return self.create_order(moves[action])\n",
    "        elif len(moves) <= action < total_actions:\n",
    "            return self.create_order(switches[action - len(moves)])\n",
    "        else:\n",
    "            #print(\"‚ùå Action hors bornes ! Fallback sur move al√©atoire\")\n",
    "            return self.choose_random_move(battle)\n",
    "        \n",
    "    def predict(self, obs):\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        q_values = self.model.q_net(obs_tensor)\n",
    "        print(f\"üìä Q-values : {q_values.detach().numpy().flatten()}\")\n",
    "        action = int(torch.argmax(q_values).item())\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Mod√®le charg√© depuis embedding_1\n"
     ]
    }
   ],
   "source": [
    "opponent = NoDynamaxRandomPlayer(battle_format=\"gen8randombattle\")\n",
    "eval_env = EmbeddingTestPlayer(\n",
    "    battle_format=\"gen8randombattle\", opponent=opponent, start_challenging=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a47f9aebfe45c18ae64d407df1f723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 53 victoires sur 100 matchs\n",
      "üéØ Reward moyen : 2.27\n"
     ]
    }
   ],
   "source": [
    "n_eval_episodes = 100\n",
    "rewards = []\n",
    "wins = 0\n",
    "\n",
    "obs, _ = eval_env.reset()\n",
    "for _ in tqdm(range(n_eval_episodes)):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        total_reward += reward\n",
    "        if done and reward > 0:\n",
    "            wins += 1\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "    obs, _ = eval_env.reset()\n",
    "\n",
    "print(f\"‚úÖ {wins} victoires sur {n_eval_episodes} matchs\")\n",
    "print(f\"üéØ Reward moyen : {sum(rewards) / len(rewards):.2f}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Embedding = ['moves_base_power*moves_dmg_multiplier','pokemon_types','pokemon_stats']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) On va rajouter les talents d'immunit√©. On va rajouter un case pour les talents qui donnent des immunit√©s, voila on fait √ßa √† la mano pas grave (if ability in list_immunies_abilities ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VS me\n",
    "from DQ_simple import DQ_simple \n",
    "class DQ_simpleemb1(DQ_simple) :\n",
    "    def __init__(self, model_path = \"embedding_1\", battle_format=\"gen8randombattle\"):\n",
    "        super().__init__(battle_format=battle_format)\n",
    "        \n",
    "        self.model = DQN.load(model_path, device=\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        print(\"üì• Mod√®le DQN charg√© :\", self.model)\n",
    "    \n",
    "    def embed_battle(self, battle )  :\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        moves_real_power = -np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "            moves_real_power[i] = moves_dmg_multiplier[i]*moves_base_power[i]\n",
    "\n",
    "        #Pokemon types  \n",
    "        pokemon_types = obtain_pokemon_types(battle)\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        final_vector = np.concatenate(\n",
    "            [\n",
    "                moves_real_power,\n",
    "                pokemon_types,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return np.float32(final_vector)\n",
    "\n",
    "    def choose_move(self, battle):\n",
    "        print(\"üëâ choose_move appel√©e !\")\n",
    "        # üîç Debug : Voir les moves disponibles\n",
    "        print(f\"üîç Moves disponibles : {[move.id for move in battle.available_moves]}\")\n",
    "\n",
    "        # Obtenir l'observation de l'√©tat du combat\n",
    "        obs = self.embed_battle(battle)\n",
    "        print(\"üìä Observation de l'√©tat :\", obs)\n",
    "\n",
    "        # Transformer en format PyTorch\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        print(\"üìä Tensor pour le mod√®le :\", obs_tensor)\n",
    "\n",
    "        # Pr√©dire l'action avec le mod√®le DQN\n",
    "        action = int(self.model.predict(obs_tensor, deterministic=True)[0])\n",
    "        print(\"üéØ Action choisie par DQN :\", action)\n",
    "        if 0 <= action < len(battle.available_moves):\n",
    "            move = battle.available_moves[action]\n",
    "            print(f\"‚úÖ Move choisi : {move.id}\")\n",
    "\n",
    "            order = self.create_order(move)\n",
    "            print(f\"üì§ Ordre cr√©√© : {order}\")  # Debug : voir l'ordre exact g√©n√©r√©\n",
    "\n",
    "            return order\n",
    "        else:\n",
    "            print(f\"‚ùå Action {action} invalide, on joue un move al√©atoire !\")\n",
    "            return self.choose_random_move(battle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (showdown_3)",
   "language": "python",
   "name": "showdown_3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "455d2c1a4ef8d735c62ff6b7867677aa8f5a8db787748daaa4d34b19b9911963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
