
import poke_env
import numpy as np
from stable_baselines3.dqn.policies import DQNPolicy



def get_valid_actions(battle) -> tuple[list[int], np.ndarray]:
    """
    Retourne la liste des IDs dâ€™actions valides (0-8)
    et un masque binaire de longueur 9.
    """
    valid_action_ids = []
    action_ids_ref_dict = {}
    # RÃ©cupÃ©rer le PokÃ©mon actif
    active = battle.active_pokemon
    if not active:
        for i, p in enumerate(battle.available_switches):
            if not p.fainted:
                if 0 <= i <= 4:  # Banc de 2 Ã  6
                    action_id = 4 + i
                    valid_action_ids.append(action_id)
                    action_ids_ref_dict[i+4] = "switch to " + p.species.lower()
    else : 
    # VÃ©rification des mouvements disponibles pour l'active PokÃ©mon
        for i, move in enumerate(active.moves.values()):
            if move in battle.available_moves:
                valid_action_ids.append(i)
                action_ids_ref_dict[i] = ("move", move) 
        # Ajouter uniquement les PokÃ©mon valides du banc (pas le PokÃ©mon actif)
        for i,  p in enumerate(battle.available_switches):
            if not p.fainted:
                if 0 <= i <= 4:  # Banc de 2 Ã  6
                    action_id = 4 + i
                    valid_action_ids.append(action_id)
                    action_ids_ref_dict[i+4] = "switch to " + p.species.lower()

    # CrÃ©ation du masque
    action_mask = np.zeros(9, dtype=np.float32)

    # Filtrage : on ne prend que des indices entre 0 et 8 (valides)
    valid_action_ids = [action_id for action_id in valid_action_ids if 0 <= action_id < 9]

    # Remplir le masque avec les actions valides
    action_mask[valid_action_ids] = 1.0

    return valid_action_ids, action_mask, action_ids_ref_dict



import torch
import torch.nn.functional as F

def masked_dqn_loss(q_pred, target_q_values, action_mask):
    """
    Calcule la perte MSE uniquement sur les actions valides.
    
    Params:
    - q_pred : (batch_size, 9) â†’ Q-values prÃ©dites par le modÃ¨le
    - target_q_values : (batch_size, 9) â†’ cibles calculÃ©es (via Bellman)
    - action_mask : (batch_size, 9) â†’ 1 si action valide, 0 sinon
    
    Returns:
    - loss : scalaire, moyenne de la perte MSE sur les actions valides uniquement
    """
    # On applique le masque pour ne garder que les Q-values valides
    mask = action_mask.bool()  # transforme en masque boolÃ©en

    # Calcul de la perte uniquement sur les entrÃ©es valides
    loss = F.mse_loss(q_pred[mask], target_q_values[mask])

    return loss


class MaskedDQNPolicy(DQNPolicy):
    def compute_q_loss(self, obs, actions, rewards, next_obs, dones, weights, target_net):
        # Q-values pour lâ€™Ã©tat actuel
        q_values = self.q_net(obs)  # (batch, 9)
        # Q-values pour lâ€™Ã©tat suivant
        with torch.no_grad():
            next_q_values = target_net(next_obs)
            next_q_values = next_q_values.max(dim=1)[0]  # max_a' Q(s', a')

        # Cible Bellman
        target = rewards + (1.0 - dones) * self.gamma * next_q_values

        # Q-value pour lâ€™action prise
        q_values_taken = q_values.gather(1, actions.long().unsqueeze(1)).squeeze(1)

        # ğŸ¯ action_mask depuis les 9 premiÃ¨res colonnes de lâ€™obs
        action_mask = obs[:, :9]  # (batch, 9)
        valid = torch.gather(action_mask, 1, actions.long().unsqueeze(1)).squeeze(1)  # (batch,)
        mask = valid > 0.5  # boolÃ©en : True si action valide

        # ğŸ”¹ MSE classique sur actions valides uniquement
        loss_valid = F.mse_loss(q_values_taken[mask], target[mask])

        # ğŸ”¸ Bonus : pÃ©nalitÃ© explicite pour actions invalides
        penalty_invalid = (1.0 - valid) * 1.0  # tu peux ajuster ce 1.0
        loss_penalty = penalty_invalid.mean()

        # ğŸ¯ Total
        total_loss = loss_valid + loss_penalty
        return total_loss