{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répertoire actuel : /Users/dan2/Desktop/Télécom-master-spé/Projets_perso/Deep/Showdown_AI/my_showdown_ai_git\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ipynbname\n",
    "\n",
    "chemin_notebook = ipynbname.path()\n",
    "dossier_notebook = os.path.dirname(chemin_notebook)\n",
    "os.chdir(dossier_notebook)\n",
    "os.chdir('../../..')\n",
    "print(\"Répertoire actuel :\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from old_embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On récupère les noms, on créé des objets pokemon avec, on récupère les types et leurs one-hot encoding et on a notre dataset.\n",
    "import json\n",
    "with open(\"json/sets.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 1) Noms\n",
    "pokemon_names = list(data.keys())\n",
    "#print(pokemon_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poke_env.environment.pokemon import Pokemon\n",
    "# 2) transormer en objets\n",
    "pokemons_list = [Pokemon(gen=4, species=i) for i in pokemon_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_types_list = [obtain_one_hot_vector_type(p) for p in pokemons_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypeAutoencoder(nn.Module):\n",
    "    def __init__(self, encoded_size=4):  # taille compressée ici\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(17, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, encoded_size)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoded_size, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 17),\n",
    "            nn.Sigmoid()  # si tu normalises les entrées\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 - Loss: 2.6662\n",
      "Epoch 2/500 - Loss: 2.5974\n",
      "Epoch 3/500 - Loss: 2.5325\n",
      "Epoch 4/500 - Loss: 2.4674\n",
      "Epoch 5/500 - Loss: 2.4042\n",
      "Epoch 6/500 - Loss: 2.3383\n",
      "Epoch 7/500 - Loss: 2.2688\n",
      "Epoch 8/500 - Loss: 2.1889\n",
      "Epoch 9/500 - Loss: 2.0995\n",
      "Epoch 10/500 - Loss: 1.9875\n",
      "Epoch 11/500 - Loss: 1.8619\n",
      "Epoch 12/500 - Loss: 1.7072\n",
      "Epoch 13/500 - Loss: 1.5435\n",
      "Epoch 14/500 - Loss: 1.3688\n",
      "Epoch 15/500 - Loss: 1.1930\n",
      "Epoch 16/500 - Loss: 1.0495\n",
      "Epoch 17/500 - Loss: 0.9484\n",
      "Epoch 18/500 - Loss: 0.8755\n",
      "Epoch 19/500 - Loss: 0.8421\n",
      "Epoch 20/500 - Loss: 0.8177\n",
      "Epoch 21/500 - Loss: 0.8090\n",
      "Epoch 22/500 - Loss: 0.8204\n",
      "Epoch 23/500 - Loss: 0.8017\n",
      "Epoch 24/500 - Loss: 0.8017\n",
      "Epoch 25/500 - Loss: 0.7935\n",
      "Epoch 26/500 - Loss: 0.7992\n",
      "Epoch 27/500 - Loss: 0.7847\n",
      "Epoch 28/500 - Loss: 0.7857\n",
      "Epoch 29/500 - Loss: 0.7826\n",
      "Epoch 30/500 - Loss: 0.7864\n",
      "Epoch 31/500 - Loss: 0.7669\n",
      "Epoch 32/500 - Loss: 0.7832\n",
      "Epoch 33/500 - Loss: 0.7836\n",
      "Epoch 34/500 - Loss: 0.7752\n",
      "Epoch 35/500 - Loss: 0.7728\n",
      "Epoch 36/500 - Loss: 0.7728\n",
      "Epoch 37/500 - Loss: 0.7756\n",
      "Epoch 38/500 - Loss: 0.7667\n",
      "Epoch 39/500 - Loss: 0.7707\n",
      "Epoch 40/500 - Loss: 0.7731\n",
      "Epoch 41/500 - Loss: 0.7626\n",
      "Epoch 42/500 - Loss: 0.7740\n",
      "Epoch 43/500 - Loss: 0.7669\n",
      "Epoch 44/500 - Loss: 0.7530\n",
      "Epoch 45/500 - Loss: 0.7542\n",
      "Epoch 46/500 - Loss: 0.7514\n",
      "Epoch 47/500 - Loss: 0.7517\n",
      "Epoch 48/500 - Loss: 0.7436\n",
      "Epoch 49/500 - Loss: 0.7440\n",
      "Epoch 50/500 - Loss: 0.7402\n",
      "Epoch 51/500 - Loss: 0.7316\n",
      "Epoch 52/500 - Loss: 0.7260\n",
      "Epoch 53/500 - Loss: 0.7315\n",
      "Epoch 54/500 - Loss: 0.7261\n",
      "Epoch 55/500 - Loss: 0.7086\n",
      "Epoch 56/500 - Loss: 0.7053\n",
      "Epoch 57/500 - Loss: 0.7025\n",
      "Epoch 58/500 - Loss: 0.6996\n",
      "Epoch 59/500 - Loss: 0.6954\n",
      "Epoch 60/500 - Loss: 0.6970\n",
      "Epoch 61/500 - Loss: 0.6938\n",
      "Epoch 62/500 - Loss: 0.6950\n",
      "Epoch 63/500 - Loss: 0.6802\n",
      "Epoch 64/500 - Loss: 0.6627\n",
      "Epoch 65/500 - Loss: 0.6791\n",
      "Epoch 66/500 - Loss: 0.6549\n",
      "Epoch 67/500 - Loss: 0.6632\n",
      "Epoch 68/500 - Loss: 0.6463\n",
      "Epoch 69/500 - Loss: 0.6409\n",
      "Epoch 70/500 - Loss: 0.6275\n",
      "Epoch 71/500 - Loss: 0.6249\n",
      "Epoch 72/500 - Loss: 0.6109\n",
      "Epoch 73/500 - Loss: 0.6215\n",
      "Epoch 74/500 - Loss: 0.6100\n",
      "Epoch 75/500 - Loss: 0.5904\n",
      "Epoch 76/500 - Loss: 0.5928\n",
      "Epoch 77/500 - Loss: 0.5846\n",
      "Epoch 78/500 - Loss: 0.5782\n",
      "Epoch 79/500 - Loss: 0.5822\n",
      "Epoch 80/500 - Loss: 0.5784\n",
      "Epoch 81/500 - Loss: 0.5650\n",
      "Epoch 82/500 - Loss: 0.5534\n",
      "Epoch 83/500 - Loss: 0.5599\n",
      "Epoch 84/500 - Loss: 0.5399\n",
      "Epoch 85/500 - Loss: 0.5329\n",
      "Epoch 86/500 - Loss: 0.5519\n",
      "Epoch 87/500 - Loss: 0.5331\n",
      "Epoch 88/500 - Loss: 0.5323\n",
      "Epoch 89/500 - Loss: 0.5340\n",
      "Epoch 90/500 - Loss: 0.5284\n",
      "Epoch 91/500 - Loss: 0.5146\n",
      "Epoch 92/500 - Loss: 0.5303\n",
      "Epoch 93/500 - Loss: 0.5324\n",
      "Epoch 94/500 - Loss: 0.5359\n",
      "Epoch 95/500 - Loss: 0.5267\n",
      "Epoch 96/500 - Loss: 0.5092\n",
      "Epoch 97/500 - Loss: 0.5138\n",
      "Epoch 98/500 - Loss: 0.5086\n",
      "Epoch 99/500 - Loss: 0.5083\n",
      "Epoch 100/500 - Loss: 0.4917\n",
      "Epoch 101/500 - Loss: 0.4842\n",
      "Epoch 102/500 - Loss: 0.4921\n",
      "Epoch 103/500 - Loss: 0.4892\n",
      "Epoch 104/500 - Loss: 0.4902\n",
      "Epoch 105/500 - Loss: 0.4991\n",
      "Epoch 106/500 - Loss: 0.4888\n",
      "Epoch 107/500 - Loss: 0.4744\n",
      "Epoch 108/500 - Loss: 0.4913\n",
      "Epoch 109/500 - Loss: 0.4818\n",
      "Epoch 110/500 - Loss: 0.4773\n",
      "Epoch 111/500 - Loss: 0.4750\n",
      "Epoch 112/500 - Loss: 0.4818\n",
      "Epoch 113/500 - Loss: 0.4806\n",
      "Epoch 114/500 - Loss: 0.4615\n",
      "Epoch 115/500 - Loss: 0.4694\n",
      "Epoch 116/500 - Loss: 0.4643\n",
      "Epoch 117/500 - Loss: 0.4674\n",
      "Epoch 118/500 - Loss: 0.4689\n",
      "Epoch 119/500 - Loss: 0.4664\n",
      "Epoch 120/500 - Loss: 0.4677\n",
      "Epoch 121/500 - Loss: 0.4537\n",
      "Epoch 122/500 - Loss: 0.4693\n",
      "Epoch 123/500 - Loss: 0.4526\n",
      "Epoch 124/500 - Loss: 0.4525\n",
      "Epoch 125/500 - Loss: 0.4381\n",
      "Epoch 126/500 - Loss: 0.4519\n",
      "Epoch 127/500 - Loss: 0.4461\n",
      "Epoch 128/500 - Loss: 0.4536\n",
      "Epoch 129/500 - Loss: 0.4580\n",
      "Epoch 130/500 - Loss: 0.4602\n",
      "Epoch 131/500 - Loss: 0.4489\n",
      "Epoch 132/500 - Loss: 0.4530\n",
      "Epoch 133/500 - Loss: 0.4430\n",
      "Epoch 134/500 - Loss: 0.4420\n",
      "Epoch 135/500 - Loss: 0.4554\n",
      "Epoch 136/500 - Loss: 0.4472\n",
      "Epoch 137/500 - Loss: 0.4459\n",
      "Epoch 138/500 - Loss: 0.4428\n",
      "Epoch 139/500 - Loss: 0.4319\n",
      "Epoch 140/500 - Loss: 0.4281\n",
      "Epoch 141/500 - Loss: 0.4366\n",
      "Epoch 142/500 - Loss: 0.4354\n",
      "Epoch 143/500 - Loss: 0.4207\n",
      "Epoch 144/500 - Loss: 0.4355\n",
      "Epoch 145/500 - Loss: 0.4395\n",
      "Epoch 146/500 - Loss: 0.4224\n",
      "Epoch 147/500 - Loss: 0.4322\n",
      "Epoch 148/500 - Loss: 0.4360\n",
      "Epoch 149/500 - Loss: 0.4351\n",
      "Epoch 150/500 - Loss: 0.4208\n",
      "Epoch 151/500 - Loss: 0.4277\n",
      "Epoch 152/500 - Loss: 0.4235\n",
      "Epoch 153/500 - Loss: 0.4123\n",
      "Epoch 154/500 - Loss: 0.4151\n",
      "Epoch 155/500 - Loss: 0.4196\n",
      "Epoch 156/500 - Loss: 0.4337\n",
      "Epoch 157/500 - Loss: 0.4100\n",
      "Epoch 158/500 - Loss: 0.4195\n",
      "Epoch 159/500 - Loss: 0.4173\n",
      "Epoch 160/500 - Loss: 0.4134\n",
      "Epoch 161/500 - Loss: 0.4041\n",
      "Epoch 162/500 - Loss: 0.4090\n",
      "Epoch 163/500 - Loss: 0.4125\n",
      "Epoch 164/500 - Loss: 0.4056\n",
      "Epoch 165/500 - Loss: 0.3944\n",
      "Epoch 166/500 - Loss: 0.4144\n",
      "Epoch 167/500 - Loss: 0.4031\n",
      "Epoch 168/500 - Loss: 0.4152\n",
      "Epoch 169/500 - Loss: 0.4007\n",
      "Epoch 170/500 - Loss: 0.4085\n",
      "Epoch 171/500 - Loss: 0.3954\n",
      "Epoch 172/500 - Loss: 0.4013\n",
      "Epoch 173/500 - Loss: 0.3904\n",
      "Epoch 174/500 - Loss: 0.4132\n",
      "Epoch 175/500 - Loss: 0.4032\n",
      "Epoch 176/500 - Loss: 0.3881\n",
      "Epoch 177/500 - Loss: 0.3961\n",
      "Epoch 178/500 - Loss: 0.3973\n",
      "Epoch 179/500 - Loss: 0.3873\n",
      "Epoch 180/500 - Loss: 0.3857\n",
      "Epoch 181/500 - Loss: 0.3819\n",
      "Epoch 182/500 - Loss: 0.4041\n",
      "Epoch 183/500 - Loss: 0.3850\n",
      "Epoch 184/500 - Loss: 0.3834\n",
      "Epoch 185/500 - Loss: 0.3669\n",
      "Epoch 186/500 - Loss: 0.3751\n",
      "Epoch 187/500 - Loss: 0.3706\n",
      "Epoch 188/500 - Loss: 0.3566\n",
      "Epoch 189/500 - Loss: 0.3669\n",
      "Epoch 190/500 - Loss: 0.3735\n",
      "Epoch 191/500 - Loss: 0.3487\n",
      "Epoch 192/500 - Loss: 0.3546\n",
      "Epoch 193/500 - Loss: 0.3473\n",
      "Epoch 194/500 - Loss: 0.3592\n",
      "Epoch 195/500 - Loss: 0.3604\n",
      "Epoch 196/500 - Loss: 0.3526\n",
      "Epoch 197/500 - Loss: 0.3358\n",
      "Epoch 198/500 - Loss: 0.3314\n",
      "Epoch 199/500 - Loss: 0.3314\n",
      "Epoch 200/500 - Loss: 0.3318\n",
      "Epoch 201/500 - Loss: 0.3261\n",
      "Epoch 202/500 - Loss: 0.3281\n",
      "Epoch 203/500 - Loss: 0.3176\n",
      "Epoch 204/500 - Loss: 0.3175\n",
      "Epoch 205/500 - Loss: 0.3164\n",
      "Epoch 206/500 - Loss: 0.3226\n",
      "Epoch 207/500 - Loss: 0.3219\n",
      "Epoch 208/500 - Loss: 0.3010\n",
      "Epoch 209/500 - Loss: 0.3130\n",
      "Epoch 210/500 - Loss: 0.3145\n",
      "Epoch 211/500 - Loss: 0.3284\n",
      "Epoch 212/500 - Loss: 0.3031\n",
      "Epoch 213/500 - Loss: 0.3020\n",
      "Epoch 214/500 - Loss: 0.2929\n",
      "Epoch 215/500 - Loss: 0.2876\n",
      "Epoch 216/500 - Loss: 0.2978\n",
      "Epoch 217/500 - Loss: 0.2811\n",
      "Epoch 218/500 - Loss: 0.2856\n",
      "Epoch 219/500 - Loss: 0.2854\n",
      "Epoch 220/500 - Loss: 0.2785\n",
      "Epoch 221/500 - Loss: 0.2829\n",
      "Epoch 222/500 - Loss: 0.2907\n",
      "Epoch 223/500 - Loss: 0.2703\n",
      "Epoch 224/500 - Loss: 0.2744\n",
      "Epoch 225/500 - Loss: 0.2695\n",
      "Epoch 226/500 - Loss: 0.2709\n",
      "Epoch 227/500 - Loss: 0.2805\n",
      "Epoch 228/500 - Loss: 0.2601\n",
      "Epoch 229/500 - Loss: 0.2684\n",
      "Epoch 230/500 - Loss: 0.2715\n",
      "Epoch 231/500 - Loss: 0.2601\n",
      "Epoch 232/500 - Loss: 0.2587\n",
      "Epoch 233/500 - Loss: 0.2665\n",
      "Epoch 234/500 - Loss: 0.2557\n",
      "Epoch 235/500 - Loss: 0.2468\n",
      "Epoch 236/500 - Loss: 0.2540\n",
      "Epoch 237/500 - Loss: 0.2610\n",
      "Epoch 238/500 - Loss: 0.2506\n",
      "Epoch 239/500 - Loss: 0.2507\n",
      "Epoch 240/500 - Loss: 0.2610\n",
      "Epoch 241/500 - Loss: 0.2475\n",
      "Epoch 242/500 - Loss: 0.2572\n",
      "Epoch 243/500 - Loss: 0.2440\n",
      "Epoch 244/500 - Loss: 0.2372\n",
      "Epoch 245/500 - Loss: 0.2380\n",
      "Epoch 246/500 - Loss: 0.2410\n",
      "Epoch 247/500 - Loss: 0.2506\n",
      "Epoch 248/500 - Loss: 0.2419\n",
      "Epoch 249/500 - Loss: 0.2410\n",
      "Epoch 250/500 - Loss: 0.2418\n",
      "Epoch 251/500 - Loss: 0.2308\n",
      "Epoch 252/500 - Loss: 0.2295\n",
      "Epoch 253/500 - Loss: 0.2322\n",
      "Epoch 254/500 - Loss: 0.2238\n",
      "Epoch 255/500 - Loss: 0.2293\n",
      "Epoch 256/500 - Loss: 0.2191\n",
      "Epoch 257/500 - Loss: 0.2324\n",
      "Epoch 258/500 - Loss: 0.2298\n",
      "Epoch 259/500 - Loss: 0.2434\n",
      "Epoch 260/500 - Loss: 0.2176\n",
      "Epoch 261/500 - Loss: 0.2387\n",
      "Epoch 262/500 - Loss: 0.2330\n",
      "Epoch 263/500 - Loss: 0.2463\n",
      "Epoch 264/500 - Loss: 0.2196\n",
      "Epoch 265/500 - Loss: 0.2308\n",
      "Epoch 266/500 - Loss: 0.2202\n",
      "Epoch 267/500 - Loss: 0.2174\n",
      "Epoch 268/500 - Loss: 0.2195\n",
      "Epoch 269/500 - Loss: 0.2144\n",
      "Epoch 270/500 - Loss: 0.2320\n",
      "Epoch 271/500 - Loss: 0.2214\n",
      "Epoch 272/500 - Loss: 0.2234\n",
      "Epoch 273/500 - Loss: 0.2159\n",
      "Epoch 274/500 - Loss: 0.2189\n",
      "Epoch 275/500 - Loss: 0.2152\n",
      "Epoch 276/500 - Loss: 0.2129\n",
      "Epoch 277/500 - Loss: 0.2198\n",
      "Epoch 278/500 - Loss: 0.2031\n",
      "Epoch 279/500 - Loss: 0.2231\n",
      "Epoch 280/500 - Loss: 0.2359\n",
      "Epoch 281/500 - Loss: 0.2321\n",
      "Epoch 282/500 - Loss: 0.2094\n",
      "Epoch 283/500 - Loss: 0.2104\n",
      "Epoch 284/500 - Loss: 0.2058\n",
      "Epoch 285/500 - Loss: 0.2033\n",
      "Epoch 286/500 - Loss: 0.2127\n",
      "Epoch 287/500 - Loss: 0.2117\n",
      "Epoch 288/500 - Loss: 0.2193\n",
      "Epoch 289/500 - Loss: 0.2001\n",
      "Epoch 290/500 - Loss: 0.2026\n",
      "Epoch 291/500 - Loss: 0.2226\n",
      "Epoch 292/500 - Loss: 0.2031\n",
      "Epoch 293/500 - Loss: 0.2065\n",
      "Epoch 294/500 - Loss: 0.2034\n",
      "Epoch 295/500 - Loss: 0.2023\n",
      "Epoch 296/500 - Loss: 0.2059\n",
      "Epoch 297/500 - Loss: 0.2087\n",
      "Epoch 298/500 - Loss: 0.2008\n",
      "Epoch 299/500 - Loss: 0.2154\n",
      "Epoch 300/500 - Loss: 0.1965\n",
      "Epoch 301/500 - Loss: 0.1962\n",
      "Epoch 302/500 - Loss: 0.2058\n",
      "Epoch 303/500 - Loss: 0.2042\n",
      "Epoch 304/500 - Loss: 0.2135\n",
      "Epoch 305/500 - Loss: 0.2106\n",
      "Epoch 306/500 - Loss: 0.1967\n",
      "Epoch 307/500 - Loss: 0.1946\n",
      "Epoch 308/500 - Loss: 0.2200\n",
      "Epoch 309/500 - Loss: 0.2048\n",
      "Epoch 310/500 - Loss: 0.1971\n",
      "Epoch 311/500 - Loss: 0.1897\n",
      "Epoch 312/500 - Loss: 0.1977\n",
      "Epoch 313/500 - Loss: 0.1954\n",
      "Epoch 314/500 - Loss: 0.1989\n",
      "Epoch 315/500 - Loss: 0.1839\n",
      "Epoch 316/500 - Loss: 0.1895\n",
      "Epoch 317/500 - Loss: 0.1900\n",
      "Epoch 318/500 - Loss: 0.1843\n",
      "Epoch 319/500 - Loss: 0.1960\n",
      "Epoch 320/500 - Loss: 0.1978\n",
      "Epoch 321/500 - Loss: 0.1785\n",
      "Epoch 322/500 - Loss: 0.1935\n",
      "Epoch 323/500 - Loss: 0.1838\n",
      "Epoch 324/500 - Loss: 0.1834\n",
      "Epoch 325/500 - Loss: 0.1827\n",
      "Epoch 326/500 - Loss: 0.1816\n",
      "Epoch 327/500 - Loss: 0.1757\n",
      "Epoch 328/500 - Loss: 0.1790\n",
      "Epoch 329/500 - Loss: 0.1911\n",
      "Epoch 330/500 - Loss: 0.1910\n",
      "Epoch 331/500 - Loss: 0.1797\n",
      "Epoch 332/500 - Loss: 0.1756\n",
      "Epoch 333/500 - Loss: 0.1745\n",
      "Epoch 334/500 - Loss: 0.1929\n",
      "Epoch 335/500 - Loss: 0.1858\n",
      "Epoch 336/500 - Loss: 0.1899\n",
      "Epoch 337/500 - Loss: 0.1794\n",
      "Epoch 338/500 - Loss: 0.1767\n",
      "Epoch 339/500 - Loss: 0.1744\n",
      "Epoch 340/500 - Loss: 0.1772\n",
      "Epoch 341/500 - Loss: 0.1782\n",
      "Epoch 342/500 - Loss: 0.1813\n",
      "Epoch 343/500 - Loss: 0.1882\n",
      "Epoch 344/500 - Loss: 0.1744\n",
      "Epoch 345/500 - Loss: 0.1819\n",
      "Epoch 346/500 - Loss: 0.1741\n",
      "Epoch 347/500 - Loss: 0.1705\n",
      "Epoch 348/500 - Loss: 0.1744\n",
      "Epoch 349/500 - Loss: 0.1792\n",
      "Epoch 350/500 - Loss: 0.1757\n",
      "Epoch 351/500 - Loss: 0.1708\n",
      "Epoch 352/500 - Loss: 0.1707\n",
      "Epoch 353/500 - Loss: 0.1675\n",
      "Epoch 354/500 - Loss: 0.1767\n",
      "Epoch 355/500 - Loss: 0.1615\n",
      "Epoch 356/500 - Loss: 0.1782\n",
      "Epoch 357/500 - Loss: 0.1618\n",
      "Epoch 358/500 - Loss: 0.1721\n",
      "Epoch 359/500 - Loss: 0.1695\n",
      "Epoch 360/500 - Loss: 0.1753\n",
      "Epoch 361/500 - Loss: 0.1828\n",
      "Epoch 362/500 - Loss: 0.1698\n",
      "Epoch 363/500 - Loss: 0.1937\n",
      "Epoch 364/500 - Loss: 0.1739\n",
      "Epoch 365/500 - Loss: 0.1631\n",
      "Epoch 366/500 - Loss: 0.1580\n",
      "Epoch 367/500 - Loss: 0.1669\n",
      "Epoch 368/500 - Loss: 0.1611\n",
      "Epoch 369/500 - Loss: 0.1676\n",
      "Epoch 370/500 - Loss: 0.1666\n",
      "Epoch 371/500 - Loss: 0.1762\n",
      "Epoch 372/500 - Loss: 0.1598\n",
      "Epoch 373/500 - Loss: 0.1756\n",
      "Epoch 374/500 - Loss: 0.1659\n",
      "Epoch 375/500 - Loss: 0.1655\n",
      "Epoch 376/500 - Loss: 0.1650\n",
      "Epoch 377/500 - Loss: 0.1533\n",
      "Epoch 378/500 - Loss: 0.1525\n",
      "Epoch 379/500 - Loss: 0.1546\n",
      "Epoch 380/500 - Loss: 0.1695\n",
      "Epoch 381/500 - Loss: 0.1527\n",
      "Epoch 382/500 - Loss: 0.1680\n",
      "Epoch 383/500 - Loss: 0.1617\n",
      "Epoch 384/500 - Loss: 0.1558\n",
      "Epoch 385/500 - Loss: 0.1584\n",
      "Epoch 386/500 - Loss: 0.1634\n",
      "Epoch 387/500 - Loss: 0.1508\n",
      "Epoch 388/500 - Loss: 0.1767\n",
      "Epoch 389/500 - Loss: 0.1725\n",
      "Epoch 390/500 - Loss: 0.1656\n",
      "Epoch 391/500 - Loss: 0.1576\n",
      "Epoch 392/500 - Loss: 0.1577\n",
      "Epoch 393/500 - Loss: 0.1593\n",
      "Epoch 394/500 - Loss: 0.1631\n",
      "Epoch 395/500 - Loss: 0.1588\n",
      "Epoch 396/500 - Loss: 0.1599\n",
      "Epoch 397/500 - Loss: 0.1543\n",
      "Epoch 398/500 - Loss: 0.1640\n",
      "Epoch 399/500 - Loss: 0.1624\n",
      "Epoch 400/500 - Loss: 0.1534\n",
      "Epoch 401/500 - Loss: 0.1624\n",
      "Epoch 402/500 - Loss: 0.1458\n",
      "Epoch 403/500 - Loss: 0.1515\n",
      "Epoch 404/500 - Loss: 0.1519\n",
      "Epoch 405/500 - Loss: 0.1726\n",
      "Epoch 406/500 - Loss: 0.1609\n",
      "Epoch 407/500 - Loss: 0.1571\n",
      "Epoch 408/500 - Loss: 0.1612\n",
      "Epoch 409/500 - Loss: 0.1464\n",
      "Epoch 410/500 - Loss: 0.1594\n",
      "Epoch 411/500 - Loss: 0.1507\n",
      "Epoch 412/500 - Loss: 0.1601\n",
      "Epoch 413/500 - Loss: 0.1498\n",
      "Epoch 414/500 - Loss: 0.1490\n",
      "Epoch 415/500 - Loss: 0.1470\n",
      "Epoch 416/500 - Loss: 0.1594\n",
      "Epoch 417/500 - Loss: 0.1488\n",
      "Epoch 418/500 - Loss: 0.1685\n",
      "Epoch 419/500 - Loss: 0.1477\n",
      "Epoch 420/500 - Loss: 0.1582\n",
      "Epoch 421/500 - Loss: 0.1467\n",
      "Epoch 422/500 - Loss: 0.1638\n",
      "Epoch 423/500 - Loss: 0.1521\n",
      "Epoch 424/500 - Loss: 0.1438\n",
      "Epoch 425/500 - Loss: 0.1462\n",
      "Epoch 426/500 - Loss: 0.1523\n",
      "Epoch 427/500 - Loss: 0.1398\n",
      "Epoch 428/500 - Loss: 0.1467\n",
      "Epoch 429/500 - Loss: 0.1450\n",
      "Epoch 430/500 - Loss: 0.1404\n",
      "Epoch 431/500 - Loss: 0.1503\n",
      "Epoch 432/500 - Loss: 0.1418\n",
      "Epoch 433/500 - Loss: 0.1458\n",
      "Epoch 434/500 - Loss: 0.1464\n",
      "Epoch 435/500 - Loss: 0.1610\n",
      "Epoch 436/500 - Loss: 0.1457\n",
      "Epoch 437/500 - Loss: 0.1436\n",
      "Epoch 438/500 - Loss: 0.1379\n",
      "Epoch 439/500 - Loss: 0.1389\n",
      "Epoch 440/500 - Loss: 0.1486\n",
      "Epoch 441/500 - Loss: 0.1490\n",
      "Epoch 442/500 - Loss: 0.1428\n",
      "Epoch 443/500 - Loss: 0.1539\n",
      "Epoch 444/500 - Loss: 0.1553\n",
      "Epoch 445/500 - Loss: 0.1370\n",
      "Epoch 446/500 - Loss: 0.1489\n",
      "Epoch 447/500 - Loss: 0.1545\n",
      "Epoch 448/500 - Loss: 0.1588\n",
      "Epoch 449/500 - Loss: 0.1491\n",
      "Epoch 450/500 - Loss: 0.1429\n",
      "Epoch 451/500 - Loss: 0.1437\n",
      "Epoch 452/500 - Loss: 0.1584\n",
      "Epoch 453/500 - Loss: 0.1423\n",
      "Epoch 454/500 - Loss: 0.1427\n",
      "Epoch 455/500 - Loss: 0.1360\n",
      "Epoch 456/500 - Loss: 0.1522\n",
      "Epoch 457/500 - Loss: 0.1465\n",
      "Epoch 458/500 - Loss: 0.1530\n",
      "Epoch 459/500 - Loss: 0.1483\n",
      "Epoch 460/500 - Loss: 0.1462\n",
      "Epoch 461/500 - Loss: 0.1405\n",
      "Epoch 462/500 - Loss: 0.1509\n",
      "Epoch 463/500 - Loss: 0.1362\n",
      "Epoch 464/500 - Loss: 0.1345\n",
      "Epoch 465/500 - Loss: 0.1459\n",
      "Epoch 466/500 - Loss: 0.1615\n",
      "Epoch 467/500 - Loss: 0.1463\n",
      "Epoch 468/500 - Loss: 0.1431\n",
      "Epoch 469/500 - Loss: 0.1568\n",
      "Epoch 470/500 - Loss: 0.1528\n",
      "Epoch 471/500 - Loss: 0.1505\n",
      "Epoch 472/500 - Loss: 0.1607\n",
      "Epoch 473/500 - Loss: 0.1387\n",
      "Epoch 474/500 - Loss: 0.1332\n",
      "Epoch 475/500 - Loss: 0.1510\n",
      "Epoch 476/500 - Loss: 0.1448\n",
      "Epoch 477/500 - Loss: 0.1491\n",
      "Epoch 478/500 - Loss: 0.1342\n",
      "Epoch 479/500 - Loss: 0.1380\n",
      "Epoch 480/500 - Loss: 0.1500\n",
      "Epoch 481/500 - Loss: 0.1491\n",
      "Epoch 482/500 - Loss: 0.1380\n",
      "Epoch 483/500 - Loss: 0.1436\n",
      "Epoch 484/500 - Loss: 0.1429\n",
      "Epoch 485/500 - Loss: 0.1396\n",
      "Epoch 486/500 - Loss: 0.1379\n",
      "Epoch 487/500 - Loss: 0.1372\n",
      "Epoch 488/500 - Loss: 0.1317\n",
      "Epoch 489/500 - Loss: 0.1432\n",
      "Epoch 490/500 - Loss: 0.1517\n",
      "Epoch 491/500 - Loss: 0.1381\n",
      "Epoch 492/500 - Loss: 0.1428\n",
      "Epoch 493/500 - Loss: 0.1312\n",
      "Epoch 494/500 - Loss: 0.1587\n",
      "Epoch 495/500 - Loss: 0.1475\n",
      "Epoch 496/500 - Loss: 0.1480\n",
      "Epoch 497/500 - Loss: 0.1309\n",
      "Epoch 498/500 - Loss: 0.1488\n",
      "Epoch 499/500 - Loss: 0.1474\n",
      "Epoch 500/500 - Loss: 0.1370\n"
     ]
    }
   ],
   "source": [
    "#Entrainement autoencodeur\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 1. Prépare tes données\n",
    "# Supposons que `vectors` est ta liste de vecteurs one-hot (chacun de taille 18)\n",
    "vectors_tensor = torch.tensor(pokemon_types_list, dtype=torch.float32)\n",
    "dataset = TensorDataset(vectors_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 2. Initialise ton modèle, ta loss, ton optimizer\n",
    "encodeur_type = TypeAutoencoder(encoded_size=4) #Ou 3 ?\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(encodeur_type.parameters(), lr=1e-3)\n",
    "\n",
    "# 3. Entraînement\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # batch est un tuple (input,)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = encodeur_type(inputs)\n",
    "        loss = criterion(outputs, inputs)  # reconstruction loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encodeur_type.state_dict(), \"Utils/embedding/old/type_autoencoder.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "showdown_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33e7e3701f48ebda16bcd6f46fe6335779fe7b4b5fe34aacb01951541666ec3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
